{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anuj/.virtualenvs/video-analytics-2/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import cv2,os,sys\n",
    "os.sys.path.append(os.getcwd()+'/src/')\n",
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "import ast\n",
    "\n",
    "import common\n",
    "import numpy as np\n",
    "from estimator import TfPoseEstimator\n",
    "from networks import get_graph_path, model_wh\n",
    "import multiprocessing\n",
    "import time\n",
    "import cv2\n",
    "import sys \n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = '/home/anuj/Desktop/datasets/test-1.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_detector_without_fps_logic(video_path, input_shape, channel=2, zoom=1, model_name='mobilenet_thin', increase_fps=False):\n",
    "    \n",
    "    channel = channel\n",
    "    zoom = zoom\n",
    "    im_w, im_h = input_shape\n",
    "    e = TfPoseEstimator(get_graph_path('mobilenet_thin'), target_size=(im_w, im_h))\n",
    "    fps = 20\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    n_row = 2\n",
    "    while True:\n",
    "        if increase_fps:\n",
    "            for i in range(4):\n",
    "                cap.read()\n",
    "            \n",
    "        fps_time = time.time()\n",
    "        \n",
    "        ret, image = cap.read()\n",
    "         \n",
    "        if zoom < 1.0:\n",
    "            canvas = np.zeros_like(image)\n",
    "            img_scaled = cv2.resize(image, None, fx=zoom, fy=zoom, interpolation=cv2.INTER_LINEAR)\n",
    "            dx = (canvas.shape[1] - img_scaled.shape[1]) // 2\n",
    "            dy = (canvas.shape[0] - img_scaled.shape[0]) // 2\n",
    "            canvas[dy:dy + img_scaled.shape[0], dx:dx + img_scaled.shape[1]] = img_scaled\n",
    "            image = canvas\n",
    "        elif zoom > 1.0:\n",
    "            img_scaled = cv2.resize(image, None, fx=zoom, fy=zoom, interpolation=cv2.INTER_LINEAR)\n",
    "            dx = (img_scaled.shape[1] - image.shape[1]) // 2\n",
    "            dy = (img_scaled.shape[0] - image.shape[0]) // 2\n",
    "            image = img_scaled[dy:image.shape[0], dx:image.shape[1]]\n",
    "\n",
    "        # detect human using tf-pose\n",
    "        humans = e.inference(image)\n",
    "\n",
    "        # modified tf-pose-code\n",
    "        # get all points of a human \n",
    "        image, centers_humans = e.draw_humans(image, humans, imgcopy=False)\n",
    "\n",
    "        # find the bounding rectangle out of the human points\n",
    "        for centers in centers_humans.values():\n",
    "            rect = cv2.boundingRect(np.array( centers.values()))\n",
    "            (x,y,w,h) = rect\n",
    "            image = cv2.rectangle(image,(x,y),(x+w,y+h),(255,255,255),2)\n",
    "\n",
    "        cv2.putText(image,\n",
    "                    \"FPS: %f\" % (1.0 / (time.time() - fps_time)),\n",
    "                    (10, 10),  cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                    (0, 255, 0), 2)\n",
    "        cv2.namedWindow('tf_pose_estimation_result',cv2.WINDOW_NORMAL)\n",
    "        cv2.imshow('tf_pose_estimation_result', image)\n",
    "        fps_time = time.time()\n",
    "        if cv2.waitKey(1) == 27:\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_detector_without_fps_logic(video_path, (640, 352), increase_fps=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------- Up above is working ------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_detector_with_fps_logic(q, video_path, input_shape, channel=2, zoom=1, model_name='mobilenet_thin', increase_fps=False):\n",
    "    \"\"\"\n",
    "    model parameters: \n",
    "    \"\"\"\n",
    "    \n",
    "    channel = channel\n",
    "    zoom = zoom\n",
    "    im_w, im_h = input_shape\n",
    "    e = TfPoseEstimator(get_graph_path('mobilenet_thin'), target_size=(im_w, im_h))\n",
    "    fps = 20\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    count = 1\n",
    "    n_row = 2\n",
    "    while True:\n",
    "        if increase_fps:\n",
    "            for i in range(4):\n",
    "                cap.read()\n",
    "            \n",
    "        fps_time = time.time()\n",
    "        \n",
    "        # read n frames from 'VideoCapture' buffer\n",
    "        concatenated_frame = []\n",
    "        for row in range(n_row):\n",
    "            frame_arr = []\n",
    "            for i in range(n_row):\n",
    "                ret, frame = cap.read()\n",
    "                frame_arr.append(frame)\n",
    "\n",
    "            concat_frame = np.concatenate(frame_arr,axis=1)\n",
    "            concatenated_frame.append(concat_frame)\n",
    "\n",
    "\n",
    "        image = np.concatenate(concatenated_frame,axis=0)\n",
    "        \n",
    "        if zoom < 1.0:\n",
    "            canvas = np.zeros_like(image)\n",
    "            img_scaled = cv2.resize(image, None, fx=zoom, fy=zoom, interpolation=cv2.INTER_LINEAR)\n",
    "            dx = (canvas.shape[1] - img_scaled.shape[1]) // 2\n",
    "            dy = (canvas.shape[0] - img_scaled.shape[0]) // 2\n",
    "            canvas[dy:dy + img_scaled.shape[0], dx:dx + img_scaled.shape[1]] = img_scaled\n",
    "            image = canvas\n",
    "        elif zoom > 1.0:\n",
    "            img_scaled = cv2.resize(image, None, fx=zoom, fy=zoom, interpolation=cv2.INTER_LINEAR)\n",
    "            dx = (img_scaled.shape[1] - image.shape[1]) // 2\n",
    "            dy = (img_scaled.shape[0] - image.shape[0]) // 2\n",
    "            image = img_scaled[dy:image.shape[0], dx:image.shape[1]]\n",
    "\n",
    "        # detect human using tf-pose\n",
    "        humans = e.inference(image)\n",
    "\n",
    "        # modified tf-pose-code\n",
    "        # get all points of a human \n",
    "        image, centers_humans = e.draw_humans(image, humans, imgcopy=False)\n",
    "\n",
    "        # find the bounding rectangle out of the human points\n",
    "        for centers in centers_humans.values():\n",
    "            rect = cv2.boundingRect(np.array( centers.values()))\n",
    "            (x,y,w,h) = rect\n",
    "            image = cv2.rectangle(image,(x,y),(x+w,y+h),(255,255,255),2)\n",
    "\n",
    "            \n",
    "        ######################### DE-STITCH LOGIC #######################\n",
    "        x = 0\n",
    "        y = 0\n",
    "        destiched_frame_arr = []\n",
    "        for i in range(n_row):\n",
    "            for j in range(n_row):\n",
    "                destiched_frame_arr.append(image[y:y+im_h, x:x+im_w])\n",
    "                x += im_w\n",
    "            y += im_h\n",
    "            x = 0\n",
    "\n",
    "\n",
    "        for i in range(len(destiched_frame_arr)):\n",
    "            print('filling queue: ', count)\n",
    "            \n",
    "            q.put(frame)\n",
    "#             cv2.imshow('final', destiched_frame_arr[i])\n",
    "#             out.write(destiched_frame_arr[i])\n",
    "        count += 1\n",
    "        ######################### DE-STITCH LOGIC #######################\n",
    "        \n",
    "\n",
    "#         cv2.putText(image,\n",
    "#                     \"FPS: %f\" % (1.0 / (time.time() - fps_time)),\n",
    "#                     (10, 10),  cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "#                     (0, 255, 0), 2)\n",
    "\n",
    "#         cv2.namedWindow('tf_pose_estimation_result',cv2.WINDOW_NORMAL)\n",
    "#         cv2.imshow('tf_pose_estimation_result', image)\n",
    "        fps_time = time.time()\n",
    "#         if cv2.waitKey(1) == 27:\n",
    "#             break\n",
    "    \n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release()\n",
    "    # out.release()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human_detector_with_fps_logic(video_path, (640, 352), increase_fps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn2(q):\n",
    "    while True:\n",
    "        x = q.get()\n",
    "        cv2.imshow('hello', x)\n",
    "        k = cv2.waitKey(1)\n",
    "        if k == 27:\n",
    "            break\n",
    "\n",
    "    q.close()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = multiprocessing.Queue()\n",
    "\n",
    "proc1 = multiprocessing.Process(target=human_detector_with_fps_logic, args=(queue, video_path, (640, 352), False,))\n",
    "proc2 = multiprocessing.Process(target=fn2, args=(queue,))\n",
    "# proc1.daemon = True\n",
    "# proc2.daemon = True\n",
    "proc1.start()\n",
    "proc2.start()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# proc1.terminate()\n",
    "# proc2.terminate()\n",
    "\n",
    "# print('CAME HERE')\n",
    "\n",
    "queue.close()\n",
    "queue.join_thread()\n",
    "\n",
    "\n",
    "proc1.join()\n",
    "proc2.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------ up above is working ------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_intersection(a, b):\n",
    "    \"\"\"\n",
    "    a : rectangle in (x,y,w,h)\n",
    "    \"\"\"\n",
    "    x = max(a[0], b[0])\n",
    "    y = max(a[1], b[1])\n",
    "    w = min(a[0]+a[2], b[0]+b[2]) - x\n",
    "    h = min(a[1]+a[3], b[1]+b[3]) - y\n",
    "    if w<0 or h<0: \n",
    "        return False,() # or (0,0,0,0) ?\n",
    "    return True,(x, y, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intrusion_detector(video_path, input_shape, bbox=None, channel=2, zoom=1, model_name='mobilenet_thin', increase_fps=False):\n",
    "    \n",
    "    #-------------- fps increment settings ---\n",
    "    inc_fps_flag, num_frames_to_skip = (increase_fps)\n",
    "        \n",
    "    if inc_fps_flag:\n",
    "        if num_frames_to_skip < 1:\n",
    "            print('entered frames to skip is less than 1, please input a higher value')\n",
    "            sys.exit()\n",
    "        if num_frames_to_skip > 7:\n",
    "            print('entered frames to skip is more than 7, please input a value less than 7')\n",
    "            sys.exit()\n",
    "    #-------------- fps increment settings ---\n",
    "    \n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    #--------------- intrusion parameters ----------\n",
    "    bbox = bbox\n",
    "    text = 'select minimum area object \\n skip frame : press s\\nconfirmation : press c'\n",
    "    #--------------- intrusion parameters ----------\n",
    "    \n",
    "    \n",
    "    # pose estimator parameters\n",
    "    channel = channel\n",
    "    zoom = zoom\n",
    "    im_w, im_h = input_shape\n",
    "    e = TfPoseEstimator(get_graph_path('mobilenet_thin'), target_size=(im_w, im_h))\n",
    "    \n",
    "    \n",
    "    while cap.isOpened():\n",
    "        \n",
    "        if inc_fps_flag:\n",
    "            for i in range(inc_fps_flag):\n",
    "                cap.read()\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            ret, image = cap.read()\n",
    "            ####################### start  ########################\n",
    "            \n",
    "\n",
    "            #----------------------------------- ROI selector ---------------------------\n",
    "            while bbox is None:\n",
    "                cv2.namedWindow('select ROI', cv2.WINDOW_NORMAL)\n",
    "                for idx,i in enumerate(text.split('\\n')):\n",
    "                    cv2.putText(image, i, (10,40*(idx+1)), cv2.FONT_HERSHEY_PLAIN, 1,(0,255,0),1,cv2.LINE_AA)\n",
    "\n",
    "\n",
    "                k = cv2.waitKey(1)\n",
    "                cv2.imshow('select ROI', image)\n",
    "                if k==115:\n",
    "                    ##  press s\n",
    "                    bbox = None\n",
    "                    ret, image = cap.read()\n",
    "                    \n",
    "                if k==99:\n",
    "                    ## press c\n",
    "                    bbox = cv2.selectROI('select ROI', image)\n",
    "                    cv2.destroyWindow('select ROI')\n",
    "                    if not any(bbox):\n",
    "                        print(\"NO ROI SELECTED, Exiting!\")\n",
    "                        sys.exit(0)\n",
    "                    print(\"Selected ROI Coordinates: \" + str(bbox))\n",
    "            #----------------------------------- ROI selector ---------------------------\n",
    "            \n",
    "                \n",
    "            if zoom < 1.0:\n",
    "                canvas = np.zeros_like(image)\n",
    "                img_scaled = cv2.resize(image, None, fx=zoom, fy=zoom, interpolation=cv2.INTER_LINEAR)\n",
    "                dx = (canvas.shape[1] - img_scaled.shape[1]) // 2\n",
    "                dy = (canvas.shape[0] - img_scaled.shape[0]) // 2\n",
    "                canvas[dy:dy + img_scaled.shape[0], dx:dx + img_scaled.shape[1]] = img_scaled\n",
    "                image = canvas\n",
    "            elif zoom > 1.0:\n",
    "                img_scaled = cv2.resize(image, None, fx=zoom, fy=zoom, interpolation=cv2.INTER_LINEAR)\n",
    "                dx = (img_scaled.shape[1] - image.shape[1]) // 2\n",
    "                dy = (img_scaled.shape[0] - image.shape[0]) // 2\n",
    "                image = img_scaled[dy:image.shape[0], dx:image.shape[1]]\n",
    "\n",
    "\n",
    "            # detect human using tf-pose\n",
    "            humans = e.inference(image)\n",
    "            \n",
    "\n",
    "            # modified tf-pose-code\n",
    "            # get all points of a human \n",
    "            image, centers_humans = e.draw_humans(image, humans, imgcopy=False)\n",
    "            \n",
    "            rect_array = []\n",
    "            # find the bounding rectangle out of the human points\n",
    "            for centers in centers_humans.values():\n",
    "                rect = cv2.boundingRect(np.array( centers.values()))\n",
    "                rect_array.append(rect)\n",
    "                (x,y,w,h) = rect\n",
    "                image = cv2.rectangle(image,(x,y),(x+w,y+h),(255,255,255),2)\n",
    "            \n",
    "            \n",
    "            res = []\n",
    "            for rect in rect_array :\n",
    "                res.append(check_intersection(np.array(rect), np.array(bbox))[0])\n",
    "            \n",
    "            \n",
    "            if any(res):\n",
    "                num_humans = sum(res)\n",
    "                print('Number of humans' + str(num_humans))\n",
    "            else:\n",
    "                print('no humans in roi')\n",
    "            \n",
    "            \n",
    "            cv2.rectangle(image, (bbox[0],bbox[1]),(bbox[0]+bbox[2],bbox[1]+bbox[3]),(255,0,0),1)\n",
    "            ####################### end  ########################\n",
    "            cv2.imshow('intrusion', image)\n",
    "            k = cv2.waitKey(1)\n",
    "            if k == 27:\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected ROI Coordinates: (288, 124, 89, 183)\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "no humans in roi\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n",
      "Number of humans1\n"
     ]
    }
   ],
   "source": [
    "intrusion_detector(video_path, input_shape=(640, 352), increase_fps=(True, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
