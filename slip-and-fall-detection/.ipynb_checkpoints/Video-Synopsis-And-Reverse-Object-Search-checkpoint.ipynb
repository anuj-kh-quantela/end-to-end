{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Analytics: Video Synopsis, Reverse Object Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this document we present the working of our software for Video Synopsis and Reverse Object Search use-cases and show how our API works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Video Synopsis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What is Video Synopsis?__\n",
    "* It is basically __summarizing__ a video sequence into a smaller sequence.\n",
    "* It is done by intelligently cutting the video into user given parts and merging each part together parallely for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reverse Object Search (search an object already present in a video using user input string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What is Reverse Object Search?__\n",
    "* It is essentially searching for a particular object which is already present in a given input video.\n",
    "* The pre-processing part of out API prepares a list of objects. This can be utilized for our object search purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to call API functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Call class imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (video_synopsis.py, line 607)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/anuj/.virtualenvs/video-analytics-3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2910\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-9c105ea45a64>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from video_synopsis import VideoSynopsis\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/anuj/git/work/video-analytics/video-analytics-api/city/bangalore/video_synopsis/video_synopsis.py\"\u001b[0;36m, line \u001b[0;32m607\u001b[0m\n\u001b[0;31m    print(\"TOTAL memory: \" + str(%memit))\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from video_synopsis import VideoSynopsis\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Set video path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video path, could be a local video as well as an RTSP feed.\n",
    "path_to_video = 'data/demoVideo.mp4' # Local video\n",
    "# path_to_video = 'rtsp://192.168.1.104:8554/demoVideo.mpg' # RTSP link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Make instance variable for Video Synopsis class with path to video as a parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function: videoSynopsis(path_to_video)\n",
    "* This function takes in a path to video file location to instantiate the VideoSynopsis class. \n",
    "* It creates an object of the VideoSynopsis class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'18 seconds'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_object = VideoSynopsis(path_to_video)\n",
    "video_object.get_video_length_util()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Preprocess input video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function: preprocess()\n",
    "This function is where following pre-processing takes place:\n",
    "1. Input video is split into candidate frames.\n",
    "2. Darknet YOLO Object detection is run on these frames. \n",
    "3. Deep Sort (with Tensorflow backend) is run to generate features for tracking objects.\n",
    "4. It is also used for Time Tagging the tracked objects. \n",
    "\n",
    "\n",
    "### NOTE:\n",
    "__At the end of processing, we get a time tagged video and also the length of the video which will be used further for synopsis.__\n",
    "\n",
    "__The next step is to use this length to decide what should be the time chunk value to be used for video synopsis.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- Preprocessing Started -- \n",
      " \n",
      " -- 1. Splitting Video --> COMPLETED\n",
      " \n",
      " -- 2. Runing Object Detection --> COMPLETED\n",
      " \n",
      " -- 3. Generating Features for Tracking --> COMPLETED\n",
      " \n",
      " -- 4. Performing Time Tagging  --> COMPLETED\n",
      " \n",
      " -- Preprocessing Finished -- \n",
      "\n",
      "Time tagged video written at:  intermediate_output/demoVideo/timeTagged-demoVideo.mp4\n",
      "\n",
      "TOTAL LENGTH OF THE VIDEO IS: 18 seconds\n"
     ]
    }
   ],
   "source": [
    "video_object.preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Set time spilts or number of user defined splits\n",
    "#### Note: \n",
    "* The time splits can only be in three metrics: __seconds, minutes, or hours__ depending upon what is the length gotten while preprocessing the video\n",
    "* Number of splits can be user defined\n",
    "* The API expects these metrics in the form of a string so it is advisable to make it as a __dropdown list__ in the webpage on Atlantis platform.\n",
    "* Time (in numbers only) can be input by a user on the UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. Run Video Synopsis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function: video_synopsis(time_split, user_def_splits)\n",
    "This function requires time in seconds, minutes, or hours in which the video has to be split and merged.\n",
    "\n",
    "We can also input the number of splits we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split using time to split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time for which the video is to be synopsyzed\n",
    "time_split = \"9 seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of splits determined:  2\n",
      "Running video synopsis for  9 seconds\n",
      "\n",
      "Synopsized video written at:  /home/ubuntu/notebook/video-analytics/city/bangalore/video-synopsis-new/output/bangalore/video-synopsis-new/demoVideo/2017_12_06_17_23_19_merged-timeTagged-demoVideo.mp4\n"
     ]
    }
   ],
   "source": [
    "output_video_path = video_object.video_synopsis(time_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### path to synopsis video output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://54.164.142.129:8888/notebooks/city/bangalore/video-synopsis-new/output/bangalore/video-synopsis-new/demoVideo/2017_12_06_17_23_19_merged-timeTagged-demoVideo.mp4\n"
     ]
    }
   ],
   "source": [
    "notebook_path = output_video_path.split('/')\n",
    "print('https://'+urlopen('http://ip.42.pl/raw').read().decode(\"utf-8\")+ ':8888/notebooks/'+'/'.join(notebook_path[notebook_path.index('video-analytics')+1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split using number of splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Synopsized video written at:  /home/ubuntu/notebook/video-analytics/city/bangalore/video-synopsis-new/output/bangalore/video-synopsis-new/demoVideo/2017_12_06_17_23_19_merged-timeTagged-demoVideo.mp4\n"
     ]
    }
   ],
   "source": [
    "output_video_path = video_object.video_synopsis(user_def_split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://54.164.142.129:8888/notebooks/city/bangalore/video-synopsis-new/output/bangalore/video-synopsis-new/demoVideo/2017_12_06_17_23_19_merged-timeTagged-demoVideo.mp4\n"
     ]
    }
   ],
   "source": [
    "notebook_path = output_video_path.split('/')\n",
    "print('https://'+urlopen('http://ip.42.pl/raw').read().decode(\"utf-8\")+ ':8888/notebooks/'+'/'.join(notebook_path[notebook_path.index('video-analytics')+1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output\n",
    "* Expected output after running Video Synopsis Core API function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"merged-frame.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"merged-frame.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remaining steps for Reverse Object Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7. Get the number of unique objects present overall in the input video\n",
    "* It is advisable to call the function in a  __dropdown list__ on the webpage on the Atlantis UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function: get_unique_objects()\n",
    "This function is used to fetch all the unique objects that were found in the entire video. A user can then input the object to be searched from the given list corresponding to which all the frames would be extracted and written into a video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      "Object(s) present in the given video sequence: \n",
      "['car', 'bicycle', 'person', 'truck', 'motorbike', 'bus']\n"
     ]
    }
   ],
   "source": [
    "# Get list of unique objects in the video\n",
    "video_object.get_unique_objects()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8. Select or enter the name of the object to be searched\n",
    "#### NOTE: \n",
    "* The object should be only from the given list of objects obtained above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set query object name to be reverse searched in the video\n",
    "query_obj_name = \"bus\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9. Search the input object (string) in the video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function: reverse_object_search(query_obj_name)\n",
    "This function takes in the object name to be searched for in the entire video sequence.\n",
    "The output is another video sequence containing the subset of frames (of the original video) where the object in question was present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Searching the object: \"bus\" in demoVideo....\n",
      "\n",
      "\n",
      "Picking up the required frames and writing output to video file...\n",
      "Video has been written at: /home/ubuntu/notebook/video-analytics/city/bangalore/video-synopsis-new/output/bangalore/video-synopsis-new/demoVideo/2017_12_06_17_42_01_bus-demoVideo.mp4\n"
     ]
    }
   ],
   "source": [
    "# Reverse object search\n",
    "output_video_path = video_object.reverse_object_search(query_obj_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://54.164.142.129:8888/notebooks/city/bangalore/video-synopsis-new/output/bangalore/video-synopsis-new/demoVideo/2017_12_06_17_42_01_bus-demoVideo.mp4\n"
     ]
    }
   ],
   "source": [
    "notebook_path = output_video_path.split('/')\n",
    "print('https://'+urlopen('http://ip.42.pl/raw').read().decode(\"utf-8\")+ ':8888/notebooks/'+'/'.join(notebook_path[notebook_path.index('video-analytics')+1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Output\n",
    "* Expected output after running Reverse Object Search Core API function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"object-search.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"object-search.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of API function calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
